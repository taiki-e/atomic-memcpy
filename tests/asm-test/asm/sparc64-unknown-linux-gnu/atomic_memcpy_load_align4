asm_test::atomic_memcpy_load_align4::acquire:
 save    %sp, -240, %sp
 add     %i1, 7, %i2
 and     %i2, -8, %i4
 sub     %i4, %i1, %i2
 cmp     %i2, 64
 bgu     %xcc, .LBB8_10
 nop
 ba      .LBB8_1
 nop
.LBB8_1:
 cmp     %i2, 0
 be      %xcc, .LBB8_6
 mov     64, %i3
 ba      .LBB8_2
 nop
.LBB8_2:
 add     %i1, 64, %i3
 sub     %i1, %i4, %g3
 add     %fp, 1983, %i5
 mov     %g0, %g2
 mov     %i1, %g4
.LBB8_3:
 ldub    [%g4], %g5
 stb     %g5, [%i5]
 add     %i5, 1, %i5
 add     %g3, 1, %g5
 cmp     %g5, %g3
 mov     %g2, %g3
 movcs   %xcc, 1, %g3
 cmp     %g3, 0
 add     %g4, 1, %g4
 be      .LBB8_3
 mov     %g5, %g3
 sub     %i3, %i4, %i3
 cmp     %i3, 8
 bcs     %xcc, .LBB8_5
 nop
 ba      .LBB8_6
 nop
.LBB8_6:
 add     %fp, 1983, %i4
.LBB8_7:
 ldx     [%i1+%i2], %i5
 add     %i4, %i2, %g2
 srlx    %i5, 56, %g3
 stb     %g3, [%i4+%i2]
 stb     %i5, [%g2+7]
 srlx    %i5, 8, %g3
 stb     %g3, [%g2+6]
 srlx    %i5, 16, %g3
 stb     %g3, [%g2+5]
 srlx    %i5, 24, %g3
 stb     %g3, [%g2+4]
 srlx    %i5, 32, %g3
 stb     %g3, [%g2+3]
 srlx    %i5, 40, %g3
 stb     %g3, [%g2+2]
 srlx    %i5, 48, %i5
 stb     %i5, [%g2+1]
 add     %i3, -8, %i3
 cmp     %i3, 7
 bgu     %xcc, .LBB8_7
 add     %i2, 8, %i2
 ba      .LBB8_5
 nop
.LBB8_5:
 cmp     %i3, 0
 be      %xcc, .LBB8_11
 nop
 ba      .LBB8_8
 nop
.LBB8_8:
 add     %fp, 1983, %i4
 add     %i4, %i2, %i4
 add     %i1, %i2, %i1
.LBB8_9:
 ldub    [%i1], %i2
 stb     %i2, [%i4]
 add     %i3, -1, %i3
 add     %i4, 1, %i4
 cmp     %i3, 0
 bne     %xcc, .LBB8_9
 add     %i1, 1, %i1
 ba      .LBB8_11
 nop
.LBB8_10:
 ld      [%i1], %i2
 st      %i2, [%fp+1983]
 ld      [%i1+4], %i2
 add     %fp, 1983, %i3
 or      %i3, 4, %i3
 st      %i2, [%i3]
 ld      [%i1+8], %i2
 st      %i2, [%fp+1991]
 ld      [%i1+12], %i2
 st      %i2, [%fp+1995]
 ld      [%i1+16], %i2
 st      %i2, [%fp+1999]
 ld      [%i1+20], %i2
 st      %i2, [%fp+2003]
 ld      [%i1+24], %i2
 st      %i2, [%fp+2007]
 ld      [%i1+28], %i2
 st      %i2, [%fp+2011]
 ld      [%i1+32], %i2
 st      %i2, [%fp+2015]
 ld      [%i1+36], %i2
 st      %i2, [%fp+2019]
 ld      [%i1+40], %i2
 st      %i2, [%fp+2023]
 ld      [%i1+44], %i2
 st      %i2, [%fp+2027]
 ld      [%i1+48], %i2
 st      %i2, [%fp+2031]
 ld      [%i1+52], %i2
 st      %i2, [%fp+2035]
 ld      [%i1+56], %i2
 st      %i2, [%fp+2039]
 ld      [%i1+60], %i1
 st      %i1, [%fp+2043]
.LBB8_11:
 add     %fp, 1983, %o1
 mov     64, %o2
 call    memcpy
 mov     %i0, %o0
 membar  #LoadLoad, |, #StoreLoad, |, #LoadStore, |, #StoreStore
 ret
 restore
asm_test::atomic_memcpy_load_align4::read_volatile_acquire_fence:
 save    %sp, -128, %sp
 ld      [%i1+60], %i2
 ld      [%i1+56], %i3
 ld      [%i1+52], %i4
 ld      [%i1+48], %i5
 ld      [%i1+44], %g2
 ld      [%i1+40], %g3
 ld      [%i1+36], %g4
 ld      [%i1+32], %g5
 ld      [%i1+28], %l0
 ld      [%i1+24], %l1
 ld      [%i1+20], %l2
 ld      [%i1+16], %l3
 ld      [%i1+12], %l4
 ld      [%i1+8], %l5
 ld      [%i1+4], %l6
 ld      [%i1], %i1
 st      %i1, [%i0]
 st      %l6, [%i0+4]
 st      %l5, [%i0+8]
 st      %l4, [%i0+12]
 st      %l3, [%i0+16]
 st      %l2, [%i0+20]
 st      %l1, [%i0+24]
 st      %l0, [%i0+28]
 st      %g5, [%i0+32]
 st      %g4, [%i0+36]
 st      %g3, [%i0+40]
 st      %g2, [%i0+44]
 st      %i5, [%i0+48]
 st      %i4, [%i0+52]
 st      %i3, [%i0+56]
 st      %i2, [%i0+60]
 membar  #LoadLoad, |, #StoreLoad, |, #LoadStore, |, #StoreStore
 ret
 restore
